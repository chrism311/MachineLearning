\documentclass[conference,compsoc]{IEEEtran}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{todonotes}
%\renewcommand\thesection{\Roman{section}.}
%\renewcommand\thesubsection{\thesection\Roman{subsection}}
\renewcommand\arraystretch{1.2}
\begin{document}
\author{Christian Miranda}
\title{Machine learning based EEG classification}
\maketitle

\begin{abstract}
The focus of this paper is looking into the use of machine learning to perform classification tasks involving EEG signals.
The data being used contains EEG recordings along with their corresponding label. So different classifiers were used to 
to classify said data, to its correct label. Various tools and different classifier parameters are used to seek 
out and compare which ones perform the best. 
\end{abstract}

\section{Introduction}
Using machine learning to make predictions and classify data is not a new concept, but is one that warrants further exploration. 
The University of California Irvine had conducted an experiment with EEG signals \cite{classsvm}, having subjects stay in a state of rest followed by them planning to do a voluntary movement with their right hand. 
UCI used various methods to interpret the EEG signals and in the end, classify them into one of the two labels, asleep or planning. The methods used in this work were classifiers from Scikit Learn to work on UCI's dataset to attempt to achive similar or better results. Multiple classifiers were used in order to observe which one performed the best in regards 
to accuracy. Some of the classifiers used here are similar to ones used by UCI and others that have used the same dataset \cite{fuzzy} which, the results to said tests, will be compared to the results of this report.

\section{Machine Learning Basics}
The algorithms find trends or patterns in the data and use them to make its decision-making but have to be trained first before it's able to output any results.  The patterns that it looks for is all based on the preliminary data that it is exposed to, to train itself. These algorithms are powerful tools due to the fact that the program doesn't work just by storing and retrieving data, it has a "learning" nature and can ``think" to predict future results with the data that it is given.

One task of these machine learning algorithms is classification, using them to classify the giving data based on the class's or ``labels" from the training data. 
\section{EEG Basics}
\todo{Describe the basic EEG signals}
The EEG signals are expressd in time and frequencies.  Mu waves are located in the band 7 to \SI{13}{\hertz} and Beta waves are at \SI{13}{\hertz} and above \cite{classsvm} which, studies have shown that these waves were related to planning body movement.
\section{Preprocessing}
\todo{Maybe a bit about the wavelet transform.  We will see}
%\section{UCI EEG Dataset}


\section{Materials and Methods}
We used the following processing pipeline for obtaining the results:
\begin{enumerate}
\item NumPy
\item SciPy
\item Scikit-learn
\end{enumerate}

Scikit-learn \todo{Create the citation for scikit-learn.  This can be obtained from the website} is a Python-based machine learning library that contains many tools and algorithms that are used to make predictions when introduced to new data. 

The raw dataset was extracted from UCI's repository as a text file, and by using a Python script, the data was able to be expressed as numerical arrays, ready for use by the Scikit Learn library. %Python is incapable of handling computations with matrices on its own so, NumPy was used. %NumPy is another library in Python to deal with scientific computations and calculations, similar to using MatLab.

The UCI tests dealt with acquiring EEG signals recorded from the subjects participating in the experiment.  UCI's dataset, which consists of $182$ subsets and $13$ attributes \cite{classsvm}, came from the EEG signals. $12$ of those attributes are the features and the last attribute is the label of the subset \cite{online}. 
The subjects were asked to sleep for five minutes, then wake them up, asking them to plan a movement for the following five seconds \cite{classsvm}. This was repeated over the course of about 30 minutes. Each subset is a five second fragment of the tests. The features of the samples were extracted from the EEG signals using a wavelet packet transform \cite{online}. 
Coefficients are produced by the transform, called wavelets \cite{wavelet}. These wavelet coefficients are treated as ``features" that describe the original signal.
The wavelet transform is similar to the Fourier Transform, so the features contain similar time-frequency characteristics as the FT \cite{packet}. 
The features ultimately would describe where the subsets fall in the $7-13\si{\hertz}$ band that gives insight into whether the subject is relaxed or planning a movement.
\subsection{K-Nearest Neighbor}
The first classification method used was the k-Nearest Neighbor algorithm.  This algorithm works by, in newly-introduced data, it recognizes patterns according to data around certain data points \cite{random}. The k-NN models use a user-defined parameter, $n$, that defines the number of points that the model looks out for around the tested data point. If the parameter is defined as "1," the model will search for the one point nearest to the test data and will make its assumptions to what classification it falls under. If the parameter is set to "2," the model with search for the nearest two points, if "3," so on and so forth. \todo{needs some work}  

\subsection{Support Vector Machines}
The second classifier that is being used is the Support Vector Machine. SVM models represent the data that was used to train itself, as a hyperplane \cite{classsvm}. The points of a specific class are mapped out as a group with gaps dividing all groups. So when the SVM is ran with the testing data, the data will fall under one of the groups. 

\subsection{Random Forest}
The last model used for classification is the Random Forest classifier. Random forest method is a type of Ensemble learning that uses a combination of several methods that make predictions and at the end, all the methods are used to make one final major prediction \cite{random}. When training the random forest model, it randomly creates "trees" using the training data. 
These trees are called decision trees due to the fact that they are made up of multiple decisions that are used to classify the data each tree is presented with. This is one of the user-defined parameters , $n$ estimators, that tells the classifier how many trees to make \cite{skforest}. Another parameter is "max depth," which defines how many nodes are in each trees \cite{skforest}. Apparently, many of these decision-making trees are not very helpful on their own. So the way that the classifier works is that among the many bad trees there are still few that are very useful. When the classifer is making a prediction, the date gets passed through all the trees, where each tree assigns a label on the data \cite{random}. In the end, all the values given by the trees are summed up to make the final descision. The decisions from the non-helpful trees sometimes cancel out with the others, which makes the decisions from the good models to be more prominent.

\subsection{Model Selection}
The data that is used for the classifiers has to be properly prepared for use. Out of the $182$ samples from the dataset, a certain percentage has to be set aside to train the algorithms in the classifiers, leaving the rest of the samples to be used as testing data. The reason for this is the fact that it is not practical to just use all of the data to train the classifier and then go back with the same data to test it. In that situation, you're leaving the classifier to predict something it already knows. 
So three functions in the Scikit Learn library were used to automatically split-up what in the dataset is used for training, and what is used for testing. 

K-fold is a cross-validator that splits up the data into training/test sets. A value of $k$ is given to the k-fold function which is a cross-validation parameter. The way it works is, $k$ determines the number of times the entire dataset is "folded," where each fold is validated once and the remainder are used to make the training data. 
The second cross-validator is Leave One Out. It runs through the dataset and puts one subset aside to use to test. The last cross-validator is the Cross-Validation Score function, which is fairly identical to k-fold. This was used to double check on consistency.

For k-fold and Cross-Val Score, each validation, which is again determined by the CV parameter, outputs a percentage-accuracy score. So all scores in each simulation were appended into an empty array and averaged at the end of the simulations to determine the overall accuracy of each run.

\section{Results}
\todo{mention results from the paper using the dataset.  You do this, but mention the methods that they used and whether they used cross validation or not}
Different classifiers were used to see how each one performed in terms of accuracy, since each one works differently. The reason why different data selection models 
were used was to,
\begin{description}
	\item[$\bullet$] have some data to train with and data that the classifier has not seen to use and test it with,
	\item[$\bullet$] observe that there is at least some consistency in how each classifier performs,
	\item[$\bullet$] and finally, to see how much the amount of training data used to train with, affects the accuracy.
\end{description}
\noindent With all that said, each classifer was ran numerous times at different cross validations for the k-fold and Cross-Val Score data selectors. Leave One Out leaves out 
specifically, one, subset everytime, meaning it uses the most training data.

Starting with the k-NN classifier, since each subset falls under the class of "asleep" or "planning," the nearest neighbor parameter 
was set to be "2." The idea is that every subset will look into the nearest two points and make a split decision between the two labels.
\begin{table*}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{70.33}{\percent} & \SI{66.46}{\percent} & \SI{69.21}{\percent} & \SI{70.34}{\percent} & \SI{70.38}{\percent} & \SI{69.28}{\percent} & \SI{70.4}{\percent} & \SI{69.04}{\percent} & \SI{70.79}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for k-NN Classifier}
	\label{table1}
\end{table*}
\\ \noindent Next is the SVM classifier. The SVM has a parameter called a kernel. Kernels for the SVM include Linear, Polynomial, and Radial Basis Function. 
For the RBF, it also has a input variable,  $\sigma$, and different values were experiemented with to find the best parameter. The kernel RBF with its 
variable $\sigma$ = 5, was found to be the best choice. UCI also used SVMs with the best accuracy at \SI{71.43}{\percent} \cite{classsvm}. 
The following table is the accuracies with these parameters,
\begin{table*}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{73.63}{\percent} & \SI{71.38}{\percent} & \SI{72.49}{\percent} & \SI{71.37}{\percent} & \SI{73.61}{\percent} & \SI{72.55}{\percent} & \SI{72.57}{\percent} & \SI{73.87}{\percent} & \SI{72.92}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for SVM Classifier}
	\label{table2}
\end{table*}
\\ \noindent Lastly, the Random Forest. The $n$ estimators parameter was chosen to be $30$ and the depth of each tree to be $3$. The results in experiment \cite{fuzzy}, using simulations with classifiers similar to Random Forest, resulted in accuracies of \SI{68.13}{\percent} and \SI{71.42}{\percent}.
The following table has the results for this present report, 
\begin{table*}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{71.28}{\percent} & \SI{70.5}{\percent} & \SI{70.26}{\percent} & \SI{71.37}{\percent} & \SI{71.5}{\percent} & \SI{71.15}{\percent} & \SI{71.5}{\percent} & \SI{71.55}{\percent} & \SI{71.5}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for Random Forest Classifier}
	\label{table3}
\end{table*}
\\ \noindent Due to the nature of the Random Forest, each simulation was ran 15 times, accuracies collected, and averaged.

\section{Conclusion}
From the resulting accuracies, one can observe that there is at least some consistency within the classifers. When compared to the results 
achived by UCI and the Decision Tree test \cite{fuzzy}, the results in this report are not off to be considered a failure. In fact, they are very similar results,
some even being better. The highest accuracy in this test was at \SI{73.87}{\percent} and the second best at \SI{73.63}{\percent}, both using the SVM. 
This classifier performed better than the remaining two, but I think it was not all on its own. From the results, it seems that the models perform better when 
more data is used on the classifer to train with. The Leave One Out cross-validation used all of the data to train expect for one subset, outputting the second 
best accuracy and with 20 cross-validations, the best accuracy is achived. The more cross-validations, the more folds are made in the dataset, meaning all the folds but one, are 
used to train with. In the end, it seems that more data from the same experiment would benefit the models used here. It seems 182 samples might not be enough for them and more training data, would bring better results. 
Another possible approach for better results would be exploring other methods but with these tests and the ones conducted by others, it seems the issues might come from the data and not so much the methodology.


\bibliographystyle{unsrt}
\bibliography{bib}
\end{document}