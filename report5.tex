\documentclass{report}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{graphicx}
\renewcommand\thesection{\Roman{section}.}
\renewcommand\thesubsection{\thesection\Roman{subsection}}
\renewcommand\arraystretch{1.2}
\begin{document}
\author{Christian Miranda}
\title{MIT Conference/LSAMP Report 5}
\maketitle

\begin{abstract}
The focus of this paper is looking into the use of machine learning to perform classification tasks involving EEG signals.
The data being used contains EEG recordings along with their corresponding label. So different classifiers were used to 
to classify said data, to its correct label. Various tools and different classifier parameters are used to seek 
out and compare which ones perform the best. 
\end{abstract}

\section{Introduction}
Using machine learning to make predictions and classify data is not a new concept, but is one that warrants further exploration. 
The University of California Irvine had conducted an experiment with EEG signals \cite{classsvm}, having subjects stay in a state of rest followed by them planning to do a voluntary movement with their right hand. 
UCI used various methods to interpret the EEG signals and 
in the end, classify them into one of the two labels, asleep or planning. The methods used in this work were classifiers from 
Scikit Learn, a Python-based machine learning library, to work on UCI's dataset to attempt to achive similar or better results. Multiple classifiers were used in order to observe which one performed the best in regards 
to accuracy. Some of the classifiers used here are similar to ones used by UCI and others that have used the same dataset \cite{fuzzy} which, the results to said tests, will be compared to the results of this report.

\section{UCI EEG Dataset}
The dataset that UCI aquired from the EEG signals from their experiment, consists of 182 subsets and 13 
attributes \cite{classsvm}. 12 of those attributes are the features and the last attribute is the label of the subset \cite{online}. 
The subjects were asked to sleep for five minutes, then 
wake them up, asking them to plan a movement for the following five seconds \cite{classsvm}. This 
was repeated over the course of about 30 minutes. Each subset is a five second fragment of the tests. 
Mu waves are located in the band 7 to \SI{13}{\hertz} and Beta waves are at \SI{13}{\hertz} and above \cite{classsvm} which, studies have showen that these waves were 
related to planning body movement. The features of the samples were extracted from the EEG signals using a wavelet packet transform \cite{online}. 
Coeffcients are produced by the transform, called wavelets \cite{wavelet}. These wavelet coeffcients are treated as "features" that describe the original signal.
The wavelet transform is similar to the Fourier Transform, so the features contain similar time-frequency characteristics as the FT \cite{packet}. 
The features ultimately would describe where the subsets fall in the 7 to \SI{13}{\hertz} band that gives insight into whether the subject is relaxed or planning a movement.

\section{Model Selection}
The data that is used for the classifiers has to be properly prepared for use. Out of the 182 samples from the dataset, a certain percentage has to be set aside to train the 
algorithms in the classifiers, leaving the rest of the samples to be used as testing data. The reason for this is the fact that it is not practical to just use all of the 
data to train the classifier and then go back with the same data to test it. In that situtation, you're leaving the classifier to predict something it already knows. 
So three functions in the Scikit Learn library were used to automatically split-up what in the dataset is used for training, and what is used for testing. 
\\ \\K-fold is a cross-validator that splits up the data into training/test sets. A value of "k" is given to the k-fold function which is a cross-validation parameter. 
The way it works is, "k" determines the number of times the entire dataset is "folded," where each fold is validated once and the remainder are used to make the training data. 
The second cross-validator is Leave One Out. It runs through the dataset and puts one subset aside to use to test. The last cross-validator is the Cross-Validation Score function, 
which is fairly identical to k-fold. This was used to double check on consistency.
\\ \\For k-fold and Cross-Val Score, each validation, which is again determined by the CV parameter, outputs a percentage-accuracy score. So all scores in each simulation were 
appended into an empty array and averaged at the end of the simulations to determine the overall accuracy of each run.

\section{Classifiers}
\subsection{K-Nearest Neighbor}
The first model used for classification was the k-Nearest Neighbor in the Scikit Learn library. 
This model works by, in newly-introduced data, it recognizes patterns according to data around certain data points \cite{random}. The k-NN models use a user-defined 
parameter, n, that defines the number of points that the model looks out for around the tested data point. If the parameter is defined as "1," the model will search 
for the one point nearest to the test data and will make its assumptions to what classification it falls under. If the parameter is set to "2," the model with 
search for the nearest two points, if "3," so on and so forth.  

\subsection{Support Vector Machines}
The second classifier that is being used is the Support Vector Machine. For a two label classification, SVMs try to find a 
hyperplane which separates the input space with a max margin \cite{classsvm}. 
The following are the equations to find the optimal hyperlane:
\begin{align}
	w.x_i + b \geq + 1, \text{if}\ y_i = +1 \\
	w.x_i + b \leq + 1, \text{if}\ y_i = -1
\end{align}
\\where $x_i$ is the $i^{th}$ input vector, $y_i$ is the classification label of 
the $i^{th}$ input, w is the weight of the vector which is normal to the hyperplane, 
and b is the bias \cite{classsvm}. The hyperlane is in between two margins that 
are parallel to the hyperplane. The margins are found with the following 
equation:
\begin{equation}
	w.x_i + b \leq \pm 1
\end{equation}
\\The input vectors that determine the margins are called the support vectors. 
If they are not linearly separable, a kernal function should be applied to the 
support vectors to become linearly separate in a transform space:
\begin{equation}
	K(x_i, x_j) = \varphi(x)\varphi(x_j)
\end{equation}
\\To the linearly separate them:
\begin{equation}
	f(x) = sign(\Sigma_i \alpha_i y_j \varphi(x)\varphi(x_j) + b)
\end{equation}

\subsection{Random Forest}
The last model used for classification is the Random Forest classifier. Random forest method is a type of Ensemble learning that uses a combination of several methods 
that make predictions and at the end, all the methods are used to make one final major 
prediction \cite{random}. When training the random forest model, it randomly creates "trees" using the training data. 
These trees are called decision trees due to the fact that they are made up of multiple decisions 
that are used to classify the data each tree is presented with. This is one of the user-defined parameters , n estimators, that tells the classifier how many trees 
to make \cite{skforest}. Another parameter is "max depth," which defines how many nodes are in each trees \cite{skforest}. Apparently, many of these decision-making 
trees aren't very helpful on their own. So the way that the classifer works is that among the many bad 
trees there are still few that are very useful. When the classifer is making a prediction, 
the date gets passed through all the trees, where each tree assigns a label on the data \cite{random}. In the end, all the values 
given by the trees are summed up to make the final discision. The decisions from the non-helpful 
trees sometimes cancel out with the others, which makes the decisions from the good models to be more 
prominent.

\section{Results}
Different classifiers were used to see how each one performed in terms of accuracy, since each one works differently. The reason why different data selection models 
were used was to,
\begin{description}
	\item[$\bullet$] have some data to train with and data that the classifier has not seen to use and test it with,
	\item[$\bullet$] observe that there is at least some consistency in how each classifier performs,
	\item[$\bullet$] and finally, to see how much the amount of training data used to train with, affects the accuracy.
\end{description}
\noindent With all that said, each classifer was ran numerous times at different cross validations for the k-fold and Cross-Val Score data selectors. Leave One Out leaves out 
specifically, one, subset everytime, meaning it uses the most training data.
\\ \\Starting with the k-NN classifier, since each subset falls under the class of "asleep" or "planning," the nearest neighbor parameter 
was set to be "2." The idea is that every subset will look into the nearest two points and make a split decision between the two labels.
\begin{table}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{70.33}{\percent} & \SI{66.46}{\percent} & \SI{69.21}{\percent} & \SI{70.34}{\percent} & \SI{70.38}{\percent} & \SI{69.28}{\percent} & \SI{70.4}{\percent} & \SI{69.04}{\percent} & \SI{70.79}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for k-NN Classifier}
	\label{table1}
\end{table}
\\ \noindent Next is the SVM classifier. The SVM has a parameter called a kernel. Kernels for the SVM include Linear, Polynomial, and Radial Basis Function. 
For the RBF, it also has a input variable,  $\sigma$, and different values were experiemented with to find the best parameter. The kernel RBF with its 
variable $\sigma$ = 5, was found to be the best choice. UCI also used SVMs with the best accuracy at \SI{71.43}{\percent} \cite{classsvm}. 
The following table is the accuracies with these parameters,
\begin{table}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{73.63}{\percent} & \SI{71.38}{\percent} & \SI{72.49}{\percent} & \SI{71.37}{\percent} & \SI{73.61}{\percent} & \SI{72.55}{\percent} & \SI{72.57}{\percent} & \SI{73.87}{\percent} & \SI{72.92}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for SVM Classifier}
	\label{table2}
\end{table}
\\ \noindent Lastly, the Random Forest. The n estimators parameter was chosen to be 30 and the depth of each tree to be 3. The results in experiment \cite{fuzzy}, 
using simulations with classifiers similar to Random Forest, resulted in accuracies of \SI{68.13}{\percent} and \SI{71.42}{\percent}.
The following table has the results for this present report, 
\begin{table}[h]
	\noindent\makebox[\linewidth]{
	\begin{tabular}{lccccccccc}
		\toprule
		{} & \bf{Leave One Out} & \multicolumn{4}{c}{\bf{K-Fold}} & \multicolumn{4}{c}{\bf{Cross-Val Score}} \\
		\midrule
		Cross Validation & {---} & CV = 5 & CV = 10 & CV = 15 & CV = 20 & CV = 5 & CV = 10 & CV = 15 & CV = 20 \\
		Accuracy & \SI{71.28}{\percent} & \SI{70.5}{\percent} & \SI{70.26}{\percent} & \SI{71.37}{\percent} & \SI{71.5}{\percent} & \SI{71.15}{\percent} & \SI{71.5}{\percent} & \SI{71.55}{\percent} & \SI{71.5}{\percent}\\
		\bottomrule		
	\end{tabular}}
	\caption{Accuracies for Random Forest Classifier}
	\label{table3}
\end{table}
\\ \noindent Due to the nature of the Random Forest, each simulation was ran 15 times, accuracies collected, and averaged.

\section{Conclusion}
From the resulting accuracies, one can observe that there is at least some consistency within the classifers. When compared to the results 
achived by UCI and the Decision Tree test \cite{fuzzy}, the results in this report are not off to be considered a failure. In fact, they are very similar results,
some even being better. The highest accuracy in this test was at \SI{73.87}{\percent} and the second best at \SI{73.63}{\percent}, both using the SVM. 
This classifier performed better than the remaining two, but I think it was not all on its own. From the results, it seems that the models perform better when 
more data is used on the classifer to train with. The Leave One Out cross-validation used all of the data to train expect for one subset, outputting the second 
best accuracy and with 20 cross-validations, the best accuracy is achived. The more cross-validations, the more folds are made in the dataset, meaning all the folds but one, are 
used to train with. In the end, it seems that more data from the same experiment would benefit the models used here. It seems 182 samples might not be enough for them and more training data, would bring better results. 
Another possible approach for better results would be exploring other methods but with these tests and the ones conducted by others, it seems the issues might come from the data and not so much the methodology.


\bibliographystyle{unsrt}
\bibliography{bib}
\end{document}